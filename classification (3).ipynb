{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20fffdef-3274-4ca2-9dde-81b2a39fde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: pandas in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch pandas tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2636554d-6991-442f-8360-e5e9f90fa10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (2024.7.24)\n",
      "Requirement already satisfied: requests in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr4/cs640/yash0512/.local/lib/python3.11/site-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: sympy in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch->transformers[torch]) (1.13.0)\n",
      "Requirement already satisfied: networkx in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855c1bae-d9b5-41db-a4e2-3946f2377dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /usr4/cs640/yash0512/.local/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: requests in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4432b06-d692-492b-9745-65c522a18bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20887c-0c46-4926-bddb-83e7cba57b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154f32f-6804-490b-a19d-23cdede978c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Reading preprocessed dataset...\n",
      "Analyzing review lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (716 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median review length: 37.0\n",
      "95th percentile: 232.0\n",
      "Splitting dataset...\n",
      "Training set size: 100000, Validation set size: 20000\n",
      "Tokenizing training and validation datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60f5d6cfa284f54bed444544fcd8a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c657b41e2d0d438dbd4ef5d9f2a2b6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr4/cs640/yash0512/.local/lib/python3.11/site-packages/accelerate/utils/imports.py:355: UserWarning: Intel Extension for PyTorch 2.3 needs to work with PyTorch 2.3.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 25:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.146307</td>\n",
       "      <td>0.961650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.422624</td>\n",
       "      <td>0.964550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.474910</td>\n",
       "      <td>0.965100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4749\n",
      "Validation Accuracy: 0.9651\n",
      "Pre-tokenizing all reviews...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819255babccd4336a3a8d37985395e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16314472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for inference\n",
      "Classifying all reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying batches:   0%|          | 0/31865 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/scratch/4015310.1.l40s/ipykernel_49937/662069547.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():  # Mixed precision for speed\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "Classifying batches: 100%|██████████| 31865/31865 [3:07:59<00:00,  2.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to combined_out.csv...\n",
      "Done!\n",
      "\n",
      "Top 5 products by total reviews:\n",
      "       product_id                                  product_title  \\\n",
      "12038  B00FAPF5U0                               Candy Crush Saga   \n",
      "11689  B00E8KLWB4           The Secret Society® - Hidden Mystery   \n",
      "9428   B00992CF6W                                      Minecraft   \n",
      "11480  B00DR0PDNE  Google Chromecast HDMI Streaming Media Player   \n",
      "9771   B009UX2YAC                                 Subway Surfers   \n",
      "\n",
      "       Negative_Count  Positive_Count  Total_Reviews  Positive_Percentage  \n",
      "12038            3332           40992          44324            92.482628  \n",
      "11689            2410           31315          33725            92.853966  \n",
      "9428             2750           30213          32963            91.657313  \n",
      "11480            5925           24423          30348            80.476473  \n",
      "9771              983           28430          29413            96.657940  \n",
      "\n",
      "Sample of 20 reviews for manual evaluation:\n",
      "Review: The novel was well written as are all novels by John Grisham. The characters were well defined. The ...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Cgcfggxdfgvhbccx...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Outstanding. I could NOT put this book down... I'm a 34 year old mom and this story has enough matur...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: This is a very basic e-reader which is exactly what I wanted to go with my Kindle Fire..<br />I use ...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Fairly simplistic but fun to use nonetheless.  Allows you to purchase other models....\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: I bought 3 for our family and we love it. My kids love to play their games and watch movies. I love ...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: These speakers are amazing for the price. Can't imagine what it'd be like without them. Speaker qual...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: My first Kindle three started freezing and re-booting after about one week and then the problem beca...\n",
      "Predicted Sentiment: Negative, Original Label: Negative\n",
      "\n",
      "Review: This series is excellent!  Season 2 on Amazon is well worth the money to catch up on all the drama. ...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: An amazing movie,  will be purchasing for my re-watch collection. Great for all ages. My husband and...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Accurate and fast! Also, the slim and contemporary design fits well in my batchlor pad....\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Once you buy the razor you gotta buy the blades--and that is where the money is.  So found good qual...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: If you are looking for a Biblical representation of the Exodus, don't look here. The special effects...\n",
      "Predicted Sentiment: Negative, Original Label: Negative\n",
      "\n",
      "Review: fab game just love it, the competition with friends is really good, only down fall is if you Want to...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: An often overlooked necessity of any computing setup.  Power surges are quite common, and can damage...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: It was easy to install. It fits well. It doesn't get in the way when its open.   What it doesn't hav...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: The game is a g good game for people that like minecraft...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: Two of my 6 blades broke within the first day; I was throwing it at a soft cork board. These knives ...\n",
      "Predicted Sentiment: Negative, Original Label: Negative\n",
      "\n",
      "Review: By far the best Kindle case I have ever purchased.  It works wonderfully with my Paperwhite!  LOVE, ...\n",
      "Predicted Sentiment: Positive, Original Label: Positive\n",
      "\n",
      "Review: The digital heart monitor worked great for about 6-8 weeks then it was all over the place.  Sometime...\n",
      "Predicted Sentiment: Negative, Original Label: Negative\n",
      "\n",
      "Generating bar chart for top five products...\n",
      "Bar chart saved to combined_fine_top_five_products_bar.png\n",
      "Generating scatter plot for top 1000 products...\n",
      "Scatter plot saved to combined_fine_reviews_vs_positive_percentage.png\n",
      "\n",
      "Overall positive review percentage: 86.85%\n",
      "Product with highest positive percentage: McCormick Gourmet Spices and Seasonings (100.00%)\n",
      "Overall accuracy on the dataset: 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "infile = \"preprocessed_combined.tsv\"  \n",
    "outfile = \"combined_out.csv\"\n",
    "plot_bar = \"combined_fine_top_five_products_bar.png\"\n",
    "plot_scatter = \"combined_fine_reviews_vs_positive_percentage.png\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Reading preprocessed dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(infile, sep='\\t', usecols=[\"product_id\", \"product_title\", \"review_body\", \"star_rating\"])\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {infile} not found. Please ensure the file exists in the correct directory.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df['sentiment_label'] = df['star_rating'].apply(lambda x: \"Positive\" if x >= 4 else (\"Negative\" if x <= 2 else None))\n",
    "df = df.dropna(subset=['sentiment_label'])\n",
    "df['label'] = df['sentiment_label'].map({\"Positive\": 1, \"Negative\": 0})\n",
    "\n",
    "df[\"review_body\"] = df[\"review_body\"].fillna(\"\").astype(str)\n",
    "\n",
    "print(\"Analyzing review lengths...\")\n",
    "sample_df = df.sample(10000, random_state=42)\n",
    "review_lengths = sample_df[\"review_body\"].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n",
    "print(f\"Median review length: {review_lengths.median()}\")\n",
    "print(f\"95th percentile: {review_lengths.quantile(0.95)}\")\n",
    "max_length = 512  \n",
    "train_samples = 100000  \n",
    "val_samples = 20000     \n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    train_size=train_samples,\n",
    "    test_size=val_samples,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['review_body', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['review_body', 'label']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = [str(text) for text in examples[\"review_body\"]]\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"Tokenizing training and validation datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  \n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning the model...\")\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(\"Pre-tokenizing all reviews...\")\n",
    "all_texts = df[\"review_body\"].astype(str).tolist()\n",
    "full_dataset = Dataset.from_dict({\"review_body\": all_texts})\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review_body\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "tokenized_full = full_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_full.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "batch_size = 512  \n",
    "dataloader = DataLoader(tokenized_full, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for inference\")\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "def classify_sentiment_dataloader(dataloader):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.inference_mode():  \n",
    "        for batch in tqdm(dataloader, desc=\"Classifying batches\"):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device)\n",
    "            }\n",
    "            with amp.autocast(): \n",
    "                outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            predictions = predictions.cpu().numpy() \n",
    "            labels = [\"Negative\" if pred == 0 else \"Positive\" for pred in predictions]\n",
    "            results.extend(labels)\n",
    "    return results\n",
    "\n",
    "# Classify all reviews\n",
    "print(\"Classifying all reviews...\")\n",
    "sentiments = classify_sentiment_dataloader(dataloader)\n",
    "df[\"predicted_sentiment\"] = sentiments\n",
    "\n",
    "# Filtering out any failed classifications\n",
    "df = df[df[\"predicted_sentiment\"].notna()]\n",
    "\n",
    "product_counts = df.groupby([\"product_id\", \"product_title\", \"predicted_sentiment\"]).size().unstack(fill_value=0)\n",
    "product_counts.columns = [\"Negative_Count\", \"Positive_Count\"]\n",
    "product_counts = product_counts.reset_index()\n",
    "\n",
    "product_counts[\"Total_Reviews\"] = product_counts[\"Negative_Count\"] + product_counts[\"Positive_Count\"]\n",
    "product_counts[\"Positive_Percentage\"] = (product_counts[\"Positive_Count\"] / product_counts[\"Total_Reviews\"]) * 100\n",
    "\n",
    "print(f\"Saving results to {outfile}...\")\n",
    "product_counts.to_csv(outfile, index=False)\n",
    "print(\"Done!\")\n",
    "\n",
    "# top five products by total reviews\n",
    "top_five = product_counts.sort_values(\"Total_Reviews\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 products by total reviews:\")\n",
    "print(top_five[['product_id', 'product_title', 'Negative_Count', 'Positive_Count', 'Total_Reviews', 'Positive_Percentage']])\n",
    "\n",
    "print(\"\\nSample of 20 reviews for manual evaluation:\")\n",
    "sample_reviews = df.sample(20)\n",
    "for index, row in sample_reviews.iterrows():\n",
    "    print(f\"Review: {row['review_body'][:100]}...\")\n",
    "    print(f\"Predicted Sentiment: {row['predicted_sentiment']}, Original Label: {row['sentiment_label']}\")\n",
    "    print()\n",
    "\n",
    "print(\"Generating bar chart for top five products...\")\n",
    "top_five_plot = top_five.set_index('product_title')\n",
    "top_five_plot[['Negative_Count', 'Positive_Count']].plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title('Top 5 Products by Total Reviews')\n",
    "plt.xlabel('Product Title')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_bar)\n",
    "plt.close()\n",
    "print(f\"Bar chart saved to {plot_bar}\")\n",
    "\n",
    "print(\"Generating scatter plot for top 1000 products...\")\n",
    "top_1000 = product_counts.sort_values(\"Total_Reviews\", ascending=False).head(1000)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(top_1000['Total_Reviews'], top_1000['Positive_Percentage'], alpha=0.5)\n",
    "plt.title('Total Reviews vs. Positive Percentage')\n",
    "plt.xlabel('Total Reviews')\n",
    "plt.ylabel('Positive Percentage')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.savefig(plot_scatter)\n",
    "plt.close()\n",
    "print(f\"Scatter plot saved to {plot_scatter}\")\n",
    "\n",
    "overall_positive_percentage = (df['predicted_sentiment'] == 'Positive').mean() * 100\n",
    "print(f\"\\nOverall positive review percentage: {overall_positive_percentage:.2f}%\")\n",
    "\n",
    "highest_positive = product_counts.loc[product_counts['Positive_Percentage'].idxmax()]\n",
    "print(f\"Product with highest positive percentage: {highest_positive['product_title']} ({highest_positive['Positive_Percentage']:.2f}%)\")\n",
    "\n",
    "accuracy = (df['predicted_sentiment'] == df['sentiment_label']).mean()\n",
    "print(f\"Overall accuracy on the dataset: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3626f7-685c-49b6-aa1a-2c2c7525f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
