{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d3ad5-9e7c-4799-95dd-da9551a7640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs640/yash0512/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c390c53a1c3b49488f53403b74752214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b5de08df5b46b991bebcf9400a4f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dbd8e2e5744a8ca139c020914271ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b104709fc4b28a6102b9497ba2f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7e071ffaf442dabe73e619562255aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed256dd71e004e8e81662be807b84ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedcf0449f2040a0a4757eba6db79ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c4804cc5344a6ae34f14ba308e6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b89a442c5c14a0699afa4840b668b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6a2c41a058446aa67e12a963d26957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aa2d1be489434b84e4fa62f564ac8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981cdf9fed5b4f019af09c2dcbc65262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99ba23fd5fe4744a019dbcf89a603e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd2d90e435b46a7bb7c967fc60f3321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e442f1eabdc402eb4ecba38a0f483be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c3f4c5b9544e58a171ba8eac676dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4058e591e4477baeefcf10e77097e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID: 0060193395\n",
      "Customer Summary:\n",
      "Bill Phillips will guide you through the world of weight training, cardio training and Myoplex. You can find all of the information on the body for life website! ... You can still read the inspiring stories, get LOADS of recipes literally hundreds if not thousands.\n",
      "\n",
      "Manufacturer Summary:\n",
      "Bill Phillips will guide you through the world of weight training, cardio training and Myoplex. You can find all of the information on the body for life website. Bill Phillips might do a decent job at getting you excited, but if you already know in your heart that this is what you want to do, then I suggest you dig through the website first.\n",
      "\n",
      "Key Excerpts:\n",
      "- In the beginning you will read about some inspiring stories, and then begin to learn about healthy dieting and proper exercise.\n",
      "- Yes, he does push the Myoplex gig on you quite a bit, but thats okay, afterall, it is pretty good stuff albiet a bit expensive.\n",
      "- But there is a major setback in my opinion.\n",
      "- This is the standard of many take off programs out there...bottom line you cannot lose weight without making LIFESTYLE changes peroid.\n",
      "- Dont get me wrong, this book is decent, but there is a big setback ill tell you in a minute.\n",
      "\n",
      "Aspect Sentiments:\n",
      "lost pounds lbs: Neutral\n",
      "program this results: Positive\n",
      "fitness book books: Positive\n",
      "supplements supplement expensive: Neutral\n",
      "bill bills thanks: Positive\n",
      "change life new: Positive\n",
      "he his exercise: Positive\n",
      "pictures photos before: Neutral\n",
      "he his hes: Positive\n",
      "phillips bill fitness: Positive\n",
      "motivational inspirational motivation: Positive\n",
      "book read this: Positive\n",
      "phillips bill mr: Neutral\n",
      "minutes cardio 20: Neutral\n",
      "change life your: Positive\n",
      "myoplex supplement use: Neutral\n",
      "free day whatever: Positive\n",
      "feel look great: Positive\n",
      "lost lbs program: Positive\n",
      "shakes shake meal: Neutral\n",
      "12 program week: Positive\n",
      "worked works work: Positive\n",
      "body life bodyforlife: Positive\n",
      "eas products win: Neutral\n",
      "can easy it: Positive\n",
      "program diet weight: Positive\n",
      "bfl little fitness: Positive\n",
      "diet fad diets: Negative\n",
      "shape get book: Positive\n",
      "they worth recommended: Positive\n",
      "12 weeks 12week: Positive\n",
      "meals day six: Neutral\n",
      "diet book chapter: Positive\n",
      "weight lose loose: Positive\n",
      "body life book: Positive\n",
      "plan planning works: Positive\n",
      "bodyforlife body life: Positive\n",
      "eat exercise regularly: Neutral\n",
      "he products profit: Neutral\n",
      "gym hours dedicate: Neutral\n",
      "weeks 12 months: Neutral\n",
      "sigh okay please: Neutral\n",
      "media magazine muscle: Neutral\n",
      "fitness main participants: Positive\n",
      "bought read book: Neutral\n",
      "program started read: Neutral\n",
      "results believe unbelievable: Positive\n",
      "lose weight book: Positive\n",
      "mistaken know lets: Neutral\n",
      "workouts workout intense: Positive\n",
      "health healthier healthy: Positive\n",
      "contest winners winner: Neutral\n",
      "nutrition foods furthermore: Neutral\n",
      "reviews read negative: Negative\n",
      "challenge 12week official: Positive\n",
      "common together principles: Positive\n",
      "builder bodybuilder look: Neutral\n",
      "phillips bill path: Positive\n",
      "supplements he his: Neutral\n",
      "stars star writing: Negative\n",
      "results weeks see: Positive\n",
      "training beginner beginners: Positive\n",
      "bodybuilding bodybuilders useful: Positive\n",
      "recommend highly book: Positive\n",
      "reviews negative review: Negative\n",
      "myoplex shakes powder: Neutral\n",
      "sigh excellent now: Neutral\n",
      "hungry meals never: Positive\n",
      "foundation proceeds wish: Positive\n",
      "sweet tooth couch: Negative\n",
      "regret do wont: Neutral\n",
      "yes oh answer: Positive\n",
      "fat cells cholesterol: Neutral\n",
      "dumbbells bench sets: Neutral\n",
      "dietexercise schwarzbein exercise: Positive\n",
      "protein grams source: Neutral\n",
      "discipline dicipline disciplined: Neutral\n",
      "overweight sad obvious: Negative\n",
      "terrific excellent wonderful: Positive\n",
      "shape getting get: Positive\n",
      "lifting weights lifted: Neutral\n",
      "motivation primary essential: Positive\n",
      "eas supplements them: Positive\n",
      "old young 40: Neutral\n",
      "fail successes failures: Negative\n",
      "aspartame safe 000: Neutral\n",
      "low carb highcarbohydrate: Negative\n",
      "buy book hooked: Positive\n",
      "luck good everyone: Positive\n",
      "carbs protien carb: Neutral\n",
      "his products sell: Neutral\n",
      "instance end set: Neutral\n",
      "felt emotionally spent: Positive\n",
      "look photos before: Positive\n",
      "worth effort takes: Positive\n",
      "cravings eating junk: Neutral\n",
      "phillips bill body: Neutral\n",
      "try needed it: Neutral\n",
      "goals goal vision: Positive\n",
      "why what so: Neutral\n",
      "shape grown 40: Neutral\n",
      "gym home costs: Neutral\n",
      "no exericse absolutely: Neutral\n",
      "lifted press had: Negative\n",
      "philips phillips bill: Neutral\n",
      "atkins tried jenny: Positive\n",
      "supplement phillips supplements: Neutral\n",
      "great changed program: Positive\n",
      "muscles inch gotten: Neutral\n",
      "training nutrition explanied: Positive\n",
      "\n",
      "\n",
      "Product ID: 0060254920\n",
      "Customer Summary:\n",
      "Maurice Sendak's The Wild Things Are About To Happen is one of the most popular children's books of all time. The illustrations are scary for young children. The educational value of this book is very negative. Instead of teaching of self discipline and respect for authority, this book promotes disobedience and violence.\n",
      "\n",
      "Manufacturer Summary:\n",
      "Maurice Sendak's Where Wild Things Are is a very good book about a boy who travels in his sleep to where the wild things are. It is the book against which all other childrens books are to be judged. The illustrations are scary for young children. The educational value of this book is very negative.\n",
      "\n",
      "Key Excerpts:\n",
      "- This was a favorite for all three of my children not to mention countless children I babysat before them, especially my youngest son, Max, who dressed as the King of all Wild Things his first two Halloweens.\n",
      "- Now I frequently give it as a baby shower gift, for I truly believe no child should be without it.\n",
      "- The library branch she borrowed it fromwouldnt see Max and his wild thing friends again for almost a year.\n",
      "- I dont understand how this book is on the best sellers list.\n",
      "- and Be still!\n",
      "\n",
      "Aspect Sentiments:\n",
      "max his he: Neutral\n",
      "book this great: Positive\n",
      "wild things where: Positive\n",
      "book this my: Positive\n",
      "copy my bought: Positive\n",
      "sendak maurice sendaks: Positive\n",
      "monsters scary monster: Positive\n",
      "gift for bought: Positive\n",
      "read it reading: Positive\n",
      "must book classic: Positive\n",
      "story my love: Positive\n",
      "thanks thank fun: Positive\n",
      "it hope as: Positive\n",
      "movie disappointed better: Positive\n",
      "imagination their boys: Positive\n",
      "classic big hit: Positive\n",
      "illustrations are beautiful: Positive\n",
      "imagination great creative: Positive\n",
      "children that book: Positive\n",
      "condition shipping arrived: Positive\n",
      "year old son: Positive\n",
      "favorite books my: Positive\n",
      "cover ripped paper: Neutral\n",
      "imagination children their: Positive\n",
      "why seriously lol: Neutral\n",
      "condition in book: Positive\n",
      "say what guess: Neutral\n",
      "childrens best greatest: Positive\n",
      "he read over: Positive\n",
      "rumpus start wild: Positive\n",
      "reviews review negative: Negative\n",
      "she her it: Positive\n",
      "pictures loves colorful: Positive\n",
      "favorite highlight my: Positive\n",
      "imagination imaginative healthy: Positive\n",
      "bedtime easy bed: Positive\n",
      "hardcover donate quality: Positive\n",
      "their being child: Negative\n",
      "terrible roar claws: Positive\n",
      "illustrations story drawings: Positive\n",
      "highly recommended recommend: Positive\n",
      "still warm hot: Positive\n",
      "caldecott medal winner: Positive\n",
      "illustrations artwork many: Positive\n",
      "recommend youngster twentyfour: Positive\n",
      "story wonderful tire: Positive\n",
      "favorites favorite one: Positive\n",
      "get it bought: Positive\n",
      "classic every needs: Positive\n",
      "\n",
      "\n",
      "Product ID: 0060392452\n",
      "Customer Summary:\n",
      "Customer reviews: I do not find the liberal left to be offensive or even repugnant. I would recommend any documentary by Moore, especially Fahrenheit 911, and the book Adventures in a TV Nation, over Stupid White Men. Michael Moore has held a mirror up to many white Americans.\n",
      "\n",
      "Manufacturer Summary:\n",
      "Michael Moore has held a mirror up to many white Americans. The book wasnt funny, wasnt insightful, and was pretty stupid. I would recommend any documentary by Moore, especially Fahrenheit 911, and the book Adventures in a TV Nation, over Stupid White Men.\n",
      "\n",
      "Key Excerpts:\n",
      "- Buy this book .\n",
      "- This will change my mind.\n",
      "- I do not find the liberal left to be offensive or even repugnant.\n",
      "- Thats why we can burn the flag.\n",
      "- Im not crazy about some of their ideas but my suspician is, thats what makes it America.\n",
      "\n",
      "Aspect Sentiments:\n",
      "moore michael moores: Negative\n",
      "book this read: Negative\n",
      "he his him: Negative\n",
      "gore election florida: Negative\n",
      "we our america: Negative\n",
      "white stupid men: Negative\n",
      "his he writing: Neutral\n",
      "michael michaels his: Positive\n",
      "media news the: Negative\n",
      "funny book humor: Positive\n",
      "mike mikes or: Neutral\n",
      "stupid men white: Negative\n",
      "bush george administration: Negative\n",
      "liberal conservative unamerican: Negative\n",
      "white he stupid: Negative\n",
      "parties party they: Negative\n",
      "humor he his: Positive\n",
      "funny times humor: Positive\n",
      "will change you: Negative\n",
      "my paperback copy: Positive\n",
      "humor laughing wit: Neutral\n",
      "stars give star: Positive\n",
      "money your waste: Neutral\n",
      "democrats republicans he: Negative\n",
      "education teachers school: Negative\n",
      "bowling columbine for: Positive\n",
      "communist socialist marxist: Negative\n",
      "chapter chapters first: Positive\n",
      "white whites black: Negative\n",
      "read it please: Neutral\n",
      "facts sources plenty: Neutral\n",
      "clinton clintons had: Negative\n",
      "nope maybe course: Neutral\n",
      "yowza love we: Neutral\n",
      "review reviews negative: Negative\n",
      "amazing book great: Positive\n",
      "budget billion year: Neutral\n",
      "read book have: Positive\n",
      "down put couldnt: Positive\n",
      "author anticapitalist hype: Negative\n",
      "911 before post: Neutral\n",
      "rich recession wallowing: Negative\n",
      "  : Neutral\n",
      "amen disappointment wrong: Negative\n",
      "wow check thats: Positive\n",
      "whitey kill chapter: Negative\n",
      "funny sanders pokes: Negative\n",
      "highly recommended enjoyable: Positive\n",
      "wake bye winner: Neutral\n",
      "finish finished stinks: Negative\n",
      "israel arafat palestinians: Negative\n",
      "board send community: Neutral\n",
      "downsize downturn this: Neutral\n",
      "workers business corporations: Negative\n",
      "sources endnotes footnotes: Positive\n",
      "truth accepting know: Neutral\n",
      "points good he: Positive\n",
      "why wonder reason: Neutral\n",
      "recycling plastics trash: Negative\n",
      "rant rants unsupported: Negative\n",
      "ann coulters coulter: Neutral\n",
      "speech free freedom: Neutral\n",
      "fun read enjoyable: Positive\n",
      "title nonfiction selling: Positive\n",
      "enron written before: Positive\n",
      "chomsky noam rookies: Neutral\n",
      "angry wakeup made: Positive\n",
      "great wonderful book: Positive\n",
      "recommend book library: Positive\n",
      "please  : Neutral\n",
      "end chapter men: Neutral\n",
      "say what expect: Neutral\n",
      "\n",
      "\n",
      "Product ID: 0060514558\n",
      "Customer Summary:\n",
      "Sean Hannity is one of the two halves of the hit Fox News show Hannity and Colmes. Known for his outspokenness and his relentless support of all things conservative. Hannity has penned a few books in his efforts to spread the word about his political philosophy.\n",
      "\n",
      "Manufacturer Summary:\n",
      "Sean Hannity is one of the two halves of the hit Fox News show Hannity and Colmes. Known for his outspokenness and his relentless support of all things conservative. This book is his first to be printed and sold on a wide scale, and it contains a few good points along with some spotty and sometimes outrageous logic to back its claims.\n",
      "\n",
      "Key Excerpts:\n",
      "- Way to go Sean!\n",
      "- This book is his first to be printed and sold on a wide scale, and it contains a few good points along with some spotty and sometimes outrageous logic to back its claims.On the positive side, this book accurately rings a warning bell about the risks of a growing government bureaucracy.\n",
      "- Like the book says, even if we dont agree with our enemies in other political camps, we still have to respect their right to speak their minds.This book gets a little carried away at times, particularly when it addresses the issue of terrorism.\n",
      "- This subject pops up throughout the book and while it is certainly an important topic that deserves some attention, Hannity goes a little overboard when he makes certain assessments, like saying that the best national defense is a national offense.\n",
      "- The book has lots of information that you wont see or hear in the leftwing media.\n",
      "\n",
      "Aspect Sentiments:\n",
      "hannity hannitys sean: Negative\n",
      "he his him: Negative\n",
      "book this read: Positive\n",
      "sean seans and: Positive\n",
      "liberals conservatives liberal: Negative\n",
      "rush limbaugh limbaughs: Neutral\n",
      "americans love us: Positive\n",
      "his book show: Neutral\n",
      "read american this: Positive\n",
      "book liberal liberals: Negative\n",
      "ha figure feh: Neutral\n",
      "conservative book this: Positive\n",
      "political book this: Positive\n",
      "freedom free nation: Negative\n",
      "truth know do: Neutral\n",
      "left saddam right: Negative\n",
      "reviews negative reviewers: Negative\n",
      "000 rich paying: Negative\n",
      "media news bias: Negative\n",
      "thats better say: Neutral\n",
      "oil billion barrels: Neutral\n",
      "ring let freedom: Positive\n",
      "we die iraq: Negative\n",
      "ann coulter coulters: Positive\n",
      "crikey flame hi: Neutral\n",
      "facts logic your: Neutral\n",
      "clinton osama terrorist: Negative\n",
      "911 events ride: Negative\n",
      "reagan proposed budgets: Neutral\n",
      "evil we past: Negative\n",
      "waste money dont: Negative\n",
      "stars give star: Positive\n",
      "flag wrap patriots: Negative\n",
      "guess boy wrong: Neutral\n",
      "democrat step am: Neutral\n",
      "patriotism patriot expressions: Negative\n",
      "they their ability: Negative\n",
      "continue racism blacks: Negative\n",
      "education vouchers school: Negative\n",
      "facts author fallacy: Positive\n",
      "conservative conservatives mantra: Neutral\n",
      "rational premises easier: Neutral\n",
      "seller books best: Positive\n",
      "deliver evil forward: Positive\n",
      "think hope dunno: Neutral\n",
      "hope pass spade: Positive\n",
      "correct 100 sixty: Neutral\n",
      "left wont book: Negative\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Topic modeling to identify aspects\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     aspect_sentiments \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(topics):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bertopic/_bertopic.py:472\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    469\u001b[0m     y, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guided_topic_modeling(embeddings)\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Reduce dimensionality and fit UMAP model\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m umap_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_dimensionality\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Zero-shot Topic Modeling\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_zeroshot():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bertopic/_bertopic.py:3776\u001b[0m, in \u001b[0;36mBERTopic._reduce_dimensionality\u001b[0;34m(self, embeddings, y, partial_fit)\u001b[0m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3774\u001b[0m     \u001b[38;5;66;03m# cuml umap needs y to be an numpy array\u001b[39;00m\n\u001b[1;32m   3775\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y) \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3776\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mumap_model\u001b[38;5;241m.\u001b[39mfit(embeddings)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/umap/umap_.py:2555\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite, **kwargs)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;66;03m# sklearn pairwise_distances fails for callable metric on sparse data\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m     _m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_distance_func\n\u001b[0;32m-> 2555\u001b[0m     dmat \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;66;03m# metric is numba.jit'd or not supported by sklearn,\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;66;03m# fallback to pairwise special\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_data:\n\u001b[1;32m   2561\u001b[0m         \u001b[38;5;66;03m# Get a fresh metric since we are casting to dense\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2375\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m   2373\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1893\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1890\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1930\u001b[0m, in \u001b[0;36m_pairwise_callable\u001b[0;34m(X, Y, metric, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1928\u001b[0m     x \u001b[38;5;241m=\u001b[39m X[[i], :] \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X[i]\n\u001b[1;32m   1929\u001b[0m     y \u001b[38;5;241m=\u001b[39m Y[[j], :] \u001b[38;5;28;01mif\u001b[39;00m issparse(Y) \u001b[38;5;28;01melse\u001b[39;00m Y[j]\n\u001b[0;32m-> 1930\u001b[0m     out[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;66;03m# Make symmetric\u001b[39;00m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;66;03m# NB: out += out.T will produce incorrect results\u001b[39;00m\n\u001b[1;32m   1934\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, BartTokenizer\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('preprocessed_combined.tsv', sep='\\t')\n",
    "\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=0)\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', device=1)\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda:0')\n",
    "topic_model = BERTopic(embedding_model=sentence_model)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "grouped = df.groupby('product_id')\n",
    "\n",
    "for product_id, group in grouped:\n",
    "    reviews = group['review_body'].tolist()\n",
    "    \n",
    "    # Cleaning reviews\n",
    "    cleaned_reviews = [clean_text(review) for review in reviews]\n",
    "    \n",
    "    concatenated_reviews = ' '.join(cleaned_reviews)\n",
    "    \n",
    "    # Customer Summarization\n",
    "    prompt_customer = \"Summarize the customer reviews: \"\n",
    "    input_text_customer = prompt_customer + concatenated_reviews\n",
    "    tokens = tokenizer(input_text_customer, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    input_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    customer_summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Manufacturer Summarization\n",
    "    prompt_manufacturer = \"Provide a detailed summary of the customer reviews, focusing on specific feedback and insights: \"\n",
    "    input_text_manufacturer = prompt_manufacturer + concatenated_reviews\n",
    "    tokens = tokenizer(input_text_manufacturer, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    input_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    manufacturer_summary = summarizer(input_text, max_length=200, min_length=50, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Manufacturer Insights\n",
    "    sentences = []\n",
    "    for review in cleaned_reviews:\n",
    "        sentences.extend(nltk.sent_tokenize(review))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) > 0:\n",
    "        topics, probs = topic_model.fit_transform(sentences)\n",
    "        aspect_sentiments = {}\n",
    "        \n",
    "        for topic in set(topics):\n",
    "            if topic == -1: \n",
    "                continue\n",
    "            # top 3 words as aspect name\n",
    "            aspect_name = ' '.join([word for word, _ in topic_model.get_topic(topic)[:3]])\n",
    "            topic_sentences_indices = [i for i, t in enumerate(topics) if t == topic]\n",
    "            topic_sentences = [sentences[i] for i in topic_sentences_indices]\n",
    "            \n",
    "            if topic_sentences:\n",
    "                # Sentiment analysis \n",
    "                sentiments = sentiment_pipeline(topic_sentences)\n",
    "                sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "                label_map = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}\n",
    "                for sent in sentiments:\n",
    "                    sentiment_counts[label_map[sent['label']]] += 1\n",
    "                total = sum(sentiment_counts.values())\n",
    "                if total > 0:\n",
    "                    dominant_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
    "                else:\n",
    "                    dominant_sentiment = 'neutral'\n",
    "                \n",
    "                aspect_sentiments[aspect_name] = dominant_sentiment\n",
    "        \n",
    "        #  key excerpts\n",
    "        all_sentiments = sentiment_pipeline(sentences)\n",
    "        sentiment_to_sentences = defaultdict(list)\n",
    "        for sent, sentiment in zip(sentences, all_sentiments):\n",
    "            sentiment_to_sentences[label_map[sentiment['label']]].append(sent)\n",
    "        \n",
    "        key_excerpts = []\n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            if sentiment_to_sentences[sentiment]:\n",
    "                key_excerpts.extend(sentiment_to_sentences[sentiment][:2])  \n",
    "            if len(key_excerpts) >= 5:\n",
    "                break\n",
    "        key_excerpts = key_excerpts[:5] \n",
    "    else:\n",
    "        aspect_sentiments = {}\n",
    "        key_excerpts = []\n",
    "    \n",
    "    print(f\"Product ID: {product_id}\")\n",
    "    print(\"Customer Summary:\")\n",
    "    print(customer_summary)\n",
    "    print(\"\\nManufacturer Summary:\")\n",
    "    print(manufacturer_summary)\n",
    "    print(\"\\nKey Excerpts:\")\n",
    "    for excerpt in key_excerpts:\n",
    "        print(f\"- {excerpt}\")\n",
    "    print(\"\\nAspect Sentiments:\")\n",
    "    for aspect, sentiment in aspect_sentiments.items():\n",
    "        print(f\"{aspect}: {sentiment.capitalize()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411bb8b-3481-4f9a-aecd-968ceaeb4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /usr4/cs640/yash0512/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "/scratch/4478796.1.a100/ipykernel_781363/3346365517.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Body for Life: 12 Weeks to Mental and Physical Strength\n",
      "### Customer Summary:\n",
      "Bill Phillips will guide you through the world of weight training, cardio training and Myoplex. You can find all of the information on the body for life website! ... You can still read the inspiring stories, get LOADS of recipes literally hundreds if not thousands.\n",
      "\n",
      "### Manufacturer Insights:\n",
      "- Praise: In the beginning you will read about some inspiring stories, and then begin to learn about healthy dieting and proper exercise.\n",
      "- Praise: Yes, he does push the Myoplex gig on you quite a bit, but thats okay, afterall, it is pretty good stuff albiet a bit expensive.\n",
      "- Complaint: But there is a major setback in their opinion.\n",
      "- Praise: You can find all of the information on the body for life website!\n",
      "- Praise: ... You can still read the inspiring stories, get LOADS of recipes literally hundreds if not thousands, your training journal is ready to download.\n",
      "- Sentiment: 2835 positive, 1324 negative, 2910 neutral sentences.\n",
      "\n",
      "### Star Rating: ★★★★☆ (4.2/5)\n",
      "\n",
      "### Call to Action:\n",
      "- Customers: Discover this product today!\n",
      "- Manufacturers: Leverage the feedback to enhance your product.\n",
      "\n",
      "### Feedback Prompt:\n",
      "How can we make this summary better? Share your thoughts!\n",
      "\n",
      "\n",
      "Product: Where the Wild Things Are\n",
      "### Customer Summary:\n",
      "Maurice Sendak's Where Wild Things Are is a VERY good book about a boy who travels in his sleep to where the wild things are. The book also teaches us that monsters can be conquered, that children who misbehave can be forgiven.\n",
      "\n",
      "### Manufacturer Insights:\n",
      "- Praise: This was a favorite for all three of their children not to mention countless children Customers babysat before them, especially their youngest son, Max, who dressed as the King of all Wild Things his first two Halloweens.\n",
      "- Praise: Now Customers frequently give it as a baby shower gift, for Customers truly believe no child should be without it.\n",
      "- Praise: It can be appreciated on so many different levels.\n",
      "- Praise: Small children learn the simple yet enchanting text by heart.\n",
      "- Praise: their boys loved yelling Let the wild rumpus start!\n",
      "- Sentiment: 1850 positive, 343 negative, 969 neutral sentences.\n",
      "\n",
      "### Star Rating: ★★★★☆ (4.7/5)\n",
      "\n",
      "### Call to Action:\n",
      "- Customers: Discover this product today!\n",
      "- Manufacturers: Leverage the feedback to enhance your product.\n",
      "\n",
      "### Feedback Prompt:\n",
      "How can we make this summary better? Share your thoughts!\n",
      "\n",
      "\n",
      "Product: Stupid White Men ...And Other Sorry Excuses for the State of the Nation!\n",
      "### Customer Summary:\n",
      "Michael Moore has held a mirror up to many white Americans. The book wasnt funny, wasnt insightful, and was pretty stupid. Customers would recommend any documentary by Moore, especially Fahrenheit 911.\n",
      "\n",
      "### Manufacturer Insights:\n",
      "- Complaint: Customers do not find the liberal left to be offensive or even repugnant.\n",
      "- Complaint: Thats why we can burn the flag.\n",
      "- Complaint: But if your spiritual leader is Ted Kennedy, and your toastmaster is Michael Moore, Good God Man, can you even wonder why people dont take you seriously?This guy is a comedian.\n",
      "- Praise: Buy this book .\n",
      "- Complaint: The next movie he makes on how terrible the country is, something even more critical than F911, something so offensive as to make normal, church going Dems cringe, if he takes all the profits, HIS and all of his FRIENDS and turns all the money over to the Democratic National Party, then Ill admit hes got some moxie.But until he does that, lets face it, hes just another capitalist making a buck.\n",
      "- Sentiment: 1677 positive, 3218 negative, 2583 neutral sentences.\n",
      "\n",
      "### Star Rating: ★★★☆☆ (3.6/5)\n",
      "\n",
      "### Call to Action:\n",
      "- Customers: Discover this product today!\n",
      "- Manufacturers: Leverage the feedback to enhance your product.\n",
      "\n",
      "### Feedback Prompt:\n",
      "How can we make this summary better? Share your thoughts!\n",
      "\n",
      "\n",
      "Product: Let Freedom Ring: Winning the War of Liberty over Liberalism\n",
      "### Customer Summary:\n",
      "Sean Hannity is one of the two halves of the hit Fox News show Hannity and Colmes. Known for his outspokenness and his relentless support of all things conservative. Hannity has penned a few books in his efforts to spread the word about his political philosophy. This book is his first to be printed and sold on a wide scale.\n",
      "\n",
      "### Manufacturer Insights:\n",
      "- Praise: Way to go Sean!\n",
      "- Praise: This book is his first to be printed and sold on a wide scale, and it contains a few good points along with some spotty and sometimes outrageous logic to back its claims.On the positive side, this book accurately rings a warning bell about the risks of a growing government bureaucracy.\n",
      "- Praise: It offers some good advice on expanding school choice and for dealing with social problems.\n",
      "- Praise: Hannity is also correct when he supports the first amendment guarantee of free speech for all.\n",
      "- Complaint: Like the book says, even if we dont agree with our enemies in other political camps, we still have to respect their right to speak their minds.This book gets a little carried away at times, particularly when it addresses the issue of terrorism.\n",
      "- Sentiment: 757 positive, 1818 negative, 1406 neutral sentences.\n",
      "\n",
      "### Star Rating: ★★★☆☆ (3.0/5)\n",
      "\n",
      "### Call to Action:\n",
      "- Customers: Discover this product today!\n",
      "- Manufacturers: Leverage the feedback to enhance your product.\n",
      "\n",
      "### Feedback Prompt:\n",
      "How can we make this summary better? Share your thoughts!\n",
      "\n",
      "\n",
      "Product: The Intelligent Investor: The Definitive Book on Value Investing. A Book of Practical Counsel (Revised Edition) (Collins Business Essentials)\n",
      "### Customer Summary:\n",
      "The book is lengthy and solid, as opposed to other finance books that hope to explain investment in 100200 pages. Graham gives good advice that helps to avoid the common errors most people make in the stock market. The updated commentary makes this book a more enjoyable version than previous editions.\n",
      "\n",
      "### Manufacturer Insights:\n",
      "- Praise: Grahams writing is clear, concise and levelheaded.\n",
      "- Praise: Also helpful are numerous references to online articles at various sites Customers cannot yet vouch for these links present state.Based on their understanding, Customers highly recommend this edition to anyone interested in this book.\n",
      "- Praise: Customers feel that Customers gleaned more from this annotated edition than Customers would have from the original, without having to conduct additional research.\n",
      "- Praise: This book represents all things solid and stable in the world of stock market fluctuations that can turnabout on you in a second.\n",
      "- Praise: Graham gives good advice that helps to avoid the common errors most people make in the stock market.\n",
      "- Sentiment: 1241 positive, 402 negative, 1316 neutral sentences.\n",
      "\n",
      "### Star Rating: ★★★★☆ (4.6/5)\n",
      "\n",
      "### Call to Action:\n",
      "- Customers: Discover this product today!\n",
      "- Manufacturers: Leverage the feedback to enhance your product.\n",
      "\n",
      "### Feedback Prompt:\n",
      "How can we make this summary better? Share your thoughts!\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentences) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     sentiments \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL_0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL_1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLABEL_2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     64\u001b[0m     sentiment_counts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/base.py:1360\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1357\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1358\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1359\u001b[0m     )\n\u001b[0;32m-> 1360\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:190\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    189\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1322\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1334\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:978\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    976\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 978\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    991\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         output_attentions,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, BartTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def generalize_pronouns(text):\n",
    "    text = re.sub(r'\\bI\\b', 'Customers', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bmy\\b', 'their', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def star_rating_visualization(rating):\n",
    "    stars = '★' * int(rating) + '☆' * (5 - int(rating))\n",
    "    return stars\n",
    "\n",
    "df = pd.read_csv('preprocessed_combined.tsv', sep='\\t')\n",
    "\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=0)\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', device=0)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "grouped = df.groupby('product_id')\n",
    "\n",
    "for product_id, group in grouped:\n",
    "    reviews = group['review_body'].tolist()\n",
    "    star_rating = group['star_rating'].mean() \n",
    "    \n",
    "    cleaned_reviews = [clean_text(review) for review in reviews]\n",
    "    generalized_reviews = [generalize_pronouns(review) for review in cleaned_reviews]\n",
    "    concatenated_reviews = ' '.join(generalized_reviews)\n",
    "    \n",
    "    # Customer Summary\n",
    "    prompt_customer = \"Provide a concise general overview of customer opinions about the product based on their reviews: \"\n",
    "    input_text_customer = prompt_customer + concatenated_reviews\n",
    "    tokens = tokenizer(input_text_customer, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    input_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    customer_summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Manufacturer Insights\n",
    "    sentences = []\n",
    "    for review in generalized_reviews:\n",
    "        sentences.extend(nltk.sent_tokenize(review))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if len(sentences) > 0:\n",
    "        sentiments = sentiment_pipeline(sentences)\n",
    "        label_map = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}\n",
    "        sentiment_counts = defaultdict(int)\n",
    "        for sent in sentiments:\n",
    "            sentiment_counts[label_map[sent['label']]] += 1\n",
    "        \n",
    "        key_points = []\n",
    "        for sent, sentiment in zip(sentences, sentiments):\n",
    "            if sentiment['label'] == 'LABEL_2':  \n",
    "                key_points.append(f\"Praise: {sent}\")\n",
    "            elif sentiment['label'] == 'LABEL_0':  \n",
    "                key_points.append(f\"Complaint: {sent}\")\n",
    "        \n",
    "        key_points = key_points[:5]\n",
    "    else:\n",
    "        key_points = []\n",
    "    \n",
    "    sentiment_summary = f\"Sentiment: {sentiment_counts['positive']} positive, {sentiment_counts['negative']} negative, {sentiment_counts['neutral']} neutral sentences.\"\n",
    "    \n",
    "    print(f\"Product: {group['product_title'].iloc[0]}\")\n",
    "    print(\"### Customer Summary:\")\n",
    "    print(customer_summary)\n",
    "    print(\"\\n### Manufacturer Insights:\")\n",
    "    for point in key_points:\n",
    "        print(f\"- {point}\")\n",
    "    print(f\"- {sentiment_summary}\")\n",
    "    print(f\"\\n### Star Rating: {star_rating_visualization(star_rating)} ({star_rating:.1f}/5)\")\n",
    "    print(\"\\n### Call to Action:\")\n",
    "    print(\"- Customers: Discover this product today!\")\n",
    "    print(\"- Manufacturers: Leverage the feedback to enhance your product.\")\n",
    "    print(\"\\n### Feedback Prompt:\")\n",
    "    print(\"How can we make this summary better? Share your thoughts!\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00fa76-9207-44b8-b793-d37eac604859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
