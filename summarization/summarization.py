# -*- coding: utf-8 -*-
"""Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dna0ujf50_hGQvcZMNDKOtl6q8MpXdHf
"""

import pandas as pd
from transformers import pipeline, BartTokenizer
from bs4 import BeautifulSoup
import re
import nltk
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from rouge_score import rouge_scorer
import numpy as np
from sklearn.metrics import classification_report
import torch
import os

nltk.download('punkt')

def clean_text(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r'[^a-zA-Z0-9\s\.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def generalize_pronouns(text):
    text = re.sub(r'\bI\b', 'Customers', text, flags=re.IGNORECASE)
    text = re.sub(r'\bmy\b', 'their', text, flags=re.IGNORECASE)
    return text

def star_rating_visualization(rating):
    stars = '★' * int(rating) + '☆' * (5 - int(rating))
    return stars

def check_cuda():
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"Number of GPUs: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    try:
        torch.cuda.init()
        print("CUDA initialized successfully")
    except RuntimeError as e:
        print(f"CUDA initialization failed: {e}")

def is_valid_sentence(sent, tokenizer, max_length=514):
    try:
        tokens = tokenizer(sent, truncation=True, max_length=max_length, return_tensors='pt')
        input_ids = tokens['input_ids']
        max_id = input_ids.max().item()
        vocab_size = tokenizer.vocab_size
        if max_id >= vocab_size:
            return False
        if input_ids.shape[1] > max_length:
            return False
        return True
    except Exception:
        return False

try:
    df = pd.read_csv('preprocessed_combined.tsv, sep='\t')
except FileNotFoundError:
    print("Dataset not found. Please ensure 'preprocessed_combined.tsv' exists.")
    exit(1)

check_cuda()
device = 0 if torch.cuda.is_available() else -1
if torch.cuda.is_available() and torch.cuda.device_count() > 1:
    device = 1
print(f"Device set to use: {'cuda:' + str(device) if device >= 0 else 'cpu'}")

try:
    summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=device)
except RuntimeError as e:
    print(f"Failed to initialize summarizer on GPU: {e}")
    print("Falling back to CPU")
    device = -1
    summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=device)

try:
    sentiment_pipeline = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', device=device)
except RuntimeError as e:
    print(f"Failed to initialize sentiment pipeline on GPU: {e}")
    print("Falling back to CPU")
    sentiment_pipeline = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment', device=-1)

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

rouge1_scores, rouge2_scores, rougeL_scores = [], [], []
true_labels, pred_labels = [], []

grouped = df.groupby('product_id')

for product_id, group in grouped:
    reviews = group['review_body'].tolist()
    star_rating = group['star_rating'].mean()

    cleaned_reviews = [clean_text(str(review)) for review in reviews]
    generalized_reviews = [generalize_pronouns(review) for review in cleaned_reviews]
    concatenated_reviews = ' '.join(generalized_reviews)

    sentences = nltk.sent_tokenize(concatenated_reviews)
    if sentences:
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        tfidf_scores = tfidf_matrix.sum(axis=1).A1

        top_indices = np.argsort(tfidf_scores)[::-1][:5]
        selected_sentences = [sentences[i] for i in top_indices]
        input_text = ' '.join(selected_sentences)

        prompt_customer = "Provide a concise general overview of customer opinions about the product based on their reviews: "
        input_text_customer = prompt_customer + input_text
        tokens = tokenizer(input_text_customer, return_tensors='pt', truncation=True, max_length=1024)
        input_text_final = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)
        try:
            customer_summary = summarizer(input_text_final, max_length=100, min_length=30, do_sample=False)[0]['summary_text']
        except RuntimeError as e:
            print(f"Error during summarization for product {product_id}: {e}")
            customer_summary = "Summary generation failed."
    else:
        customer_summary = "No reviews available."

    all_sentences = []
    for review in generalized_reviews:
        all_sentences.extend(nltk.sent_tokenize(review))
    all_sentences = [s.strip() for s in all_sentences if s.strip()]

    if all_sentences:
        valid_sentences = [s for s in all_sentences if is_valid_sentence(s, sentiment_pipeline.tokenizer)]
        if len(valid_sentences) < len(all_sentences):
            print(f"Filtered {len(all_sentences) - len(valid_sentences)} invalid sentences for product {product_id}")

        if valid_sentences:
            try:
                sentiments = sentiment_pipeline(valid_sentences, truncation=True, max_length=514)
                label_map = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}
                sentiment_counts = defaultdict(int)
                for sent in sentiments:
                    sentiment_counts[label_map[sent['label']]] += 1

                key_points = []
                for sent, sentiment in zip(valid_sentences, sentiments):
                    if sentiment['label'] == 'LABEL_2':
                        key_points.append(f"Praise: {sent}")
                    elif sentiment['label'] == 'LABEL_0':
                        key_points.append(f"Complaint: {sent}")
                key_points = key_points[:5]
            except Exception as e:
                print(f"Error during sentiment analysis for product {product_id}: {e}")
                key_points = []
                sentiment_counts = defaultdict(int)
                sentiment_summary = "Sentiment analysis failed."
        else:
            key_points = []
            sentiment_counts = defaultdict(int)
            sentiment_summary = "No valid sentences for sentiment analysis."
    else:
        key_points = []
        sentiment_summary = "Sentiment: 0 positive, 0 negative, 0 neutral sentences."

    if 'sentiment_summary' not in locals():
        sentiment_summary = f"Sentiment: {sentiment_counts['positive']} positive, {sentiment_counts['negative']} negative, {sentiment_counts['neutral']} neutral sentences."

    max_rating = group['star_rating'].max()
    top_reviews = group[group['star_rating'] == max_rating]
    reference_review = top_reviews.loc[top_reviews['review_body'].str.len().idxmax()]['review_body']
    reference = clean_text(str(reference_review))

    if reference and customer_summary != "No reviews available." and "failed" not in customer_summary:
        scores = scorer.score(reference, customer_summary)
        rouge1_scores.append(scores['rouge1'].fmeasure)
        rouge2_scores.append(scores['rouge2'].fmeasure)
        rougeL_scores.append(scores['rougeL'].fmeasure)

    for _, row in group.iterrows():
        review_sentences = nltk.sent_tokenize(clean_text(generalize_pronouns(str(row['review_body']))))
        if review_sentences:
            valid_review_sentences = [s for s in review_sentences if is_valid_sentence(s, sentiment_pipeline.tokenizer)]
            if valid_review_sentences:
                try:
                    review_sentiments = sentiment_pipeline(valid_review_sentences, truncation=True, max_length=514)
                    pos_count = sum(1 for s in review_sentiments if s['label'] == 'LABEL_2')
                    neg_count = sum(1 for s in review_sentiments if s['label'] == 'LABEL_0')
                    neu_count = sum(1 for s in review_sentiments if s['label'] == 'LABEL_1')
                    pred_label = 'positive' if pos_count > neg_count and pos_count > neu_count else \
                                 'negative' if neg_count > pos_count and neg_count > neu_count else 'neutral'
                except Exception as e:
                    print(f"Error during sentiment evaluation for review in product {product_id}: {e}")
                    pred_label = 'neutral'
            else:
                pred_label = 'neutral'
        else:
            pred_label = 'neutral'

        star = row['star_rating']
        true_label = 'negative' if star in [1, 2] else 'neutral' if star == 3 else 'positive' if star in [4, 5] else 'neutral'
        true_labels.append(true_label)
        pred_labels.append(pred_label)

    print(f"Product: {group['product_title'].iloc[0]}")
    print("### Customer Summary:")
    print(customer_summary)
    print("\n### Manufacturer Insights:")
    for point in key_points:
        print(f"- {point}")
    print(f"- {sentiment_summary}")
    print(f"\n### Star Rating: {star_rating_visualization(star_rating)} ({star_rating:.1f}/5)")
    print("\n### Call to Action:")
    print("- Customers: Discover this product today!")
    print("- Manufacturers: Leverage the feedback to enhance your product.")
    print("\n### Feedback Prompt:")
    print("How can we make this summary better? Share your thoughts!")
    print("\n")

if true_labels:
    print("Sentiment Classification Evaluation:")
    print(classification_report(true_labels, pred_labels, target_names=['negative', 'neutral', 'positive']))

if rouge1_scores:
    print("Summary Quality Evaluation:")
    print(f"Average ROUGE-1 F1: {sum(rouge1_scores) / len(rouge1_scores):.4f}")
    print(f"Average ROUGE-2 F1: {sum(rouge2_scores) / len(rouge2_scores):.4f}")
    print(f"Average ROUGE-L F1: {sum(rougeL_scores) / len(rougeL_scores):.4f}")